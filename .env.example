# ============================================
# Context-Aware Chatbot - Environment Config
# ============================================
# Copy this file to .env and fill in your actual values
# Never commit .env to version control!

# ============================
# LLM Provider Configuration
# ============================
# Choose your LLM provider: "openai" or "anthropic"
# Default: openai
LLM_PROVIDER=openai

# API Keys
# --------
# Get your API keys from:
# - OpenAI: https://platform.openai.com/api-keys
# - Anthropic: https://console.anthropic.com/
OPENAI_API_KEY=sk-your-openai-api-key-here
# ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here

# ============================
# Model Selection
# ============================
# OpenAI Options:
#   - gpt-4o-mini (fast, cost-effective, recommended)
#   - gpt-4o (advanced reasoning)
#   - gpt-4-turbo (previous generation)
# Anthropic Options:
#   - claude-3-5-sonnet-20241022 (best overall, recommended)
#   - claude-3-haiku-20240307 (fast, economical)
#   - claude-3-opus-20240229 (most capable)
LLM_MODEL=gpt-4o-mini

# Generation Parameters
LLM_MAX_TOKENS=1000        # Maximum tokens in response
LLM_TEMPERATURE=0.7        # Creativity (0.0-1.0): higher = more creative

# ============================
# Context Window Management
# ============================
# Maximum tokens to use from model's context window
# Note: Leave buffer for system prompt + response
MAX_CONTEXT_TOKENS=8000

# Trigger summarization when history reaches this % of max context
# Example: 0.7 = summarize at 70% capacity
SUMMARIZATION_THRESHOLD=0.7

# Always keep this many recent messages (even after summarization)
# Recommended: 3-5 messages for conversation continuity
PRESERVE_RECENT_MESSAGES=4

# Reserve tokens for model response generation
RESPONSE_BUFFER_TOKENS=1000

# ============================
# Embedding & Memory
# ============================
# Embedding provider: "openai" or "sentence_transformer"
# sentence_transformer runs locally (no API costs, privacy)
EMBEDDING_PROVIDER=sentence_transformer

# Local embedding model (if using sentence_transformer)
# Recommended: all-MiniLM-L6-v2 (fast, 384 dimensions)
EMBEDDING_MODEL=all-MiniLM-L6-v2

# Minimum relevance score for memory retrieval (0.0-1.0)
# Higher = stricter matching. Typical range: 0.3-0.6
MIN_RELEVANCE_SCORE=0.3

# Maximum number of past memories to retrieve per query
MAX_RETRIEVED_MEMORIES=3

# ============================
# Storage Paths
# ============================
# Directory for long-term conversation memory
LONG_TERM_MEMORY_PATH=./data/memory

# ============================
# Safety & Guardrails
# ============================
# Maximum input/output lengths (characters)
MAX_INPUT_LENGTH=10000
MAX_OUTPUT_LENGTH=10000

# Enable prompt injection detection
ENABLE_INJECTION_DETECTION=true

# Risk threshold for injection detection (0.0-1.0)
INJECTION_RISK_THRESHOLD=0.5

# ============================
# Logging
# ============================
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Log format: "json" or "text"
LOG_FORMAT=text
